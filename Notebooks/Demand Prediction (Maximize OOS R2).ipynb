{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part 3 - FINAL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"KxNwb5P_ChVo","executionInfo":{"status":"ok","timestamp":1623074121321,"user_tz":240,"elapsed":660,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}}},"source":["import time\n","import xgboost as xgb\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import r2_score, mean_squared_error\n","from sklearn import linear_model\n","from sklearn.neighbors import KNeighborsRegressor\n","import random\n","import warnings"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":326},"id":"07Xr_A7SD26L","executionInfo":{"status":"ok","timestamp":1623074123087,"user_tz":240,"elapsed":482,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}},"outputId":"15120310-5cfc-4315-a330-2fe45c85b284"},"source":["sales = pd.read_csv('https://raw.githubusercontent.com/hoganj15/MMA_Assignment_Data/main/Service/data_processed.csv') \n","sales.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>week</th>\n","      <th>sku</th>\n","      <th>weekly_sales</th>\n","      <th>price</th>\n","      <th>price-1</th>\n","      <th>price-2</th>\n","      <th>feat_main_page</th>\n","      <th>trend</th>\n","      <th>month_2</th>\n","      <th>month_3</th>\n","      <th>month_4</th>\n","      <th>month_5</th>\n","      <th>month_6</th>\n","      <th>month_7</th>\n","      <th>month_8</th>\n","      <th>month_9</th>\n","      <th>month_10</th>\n","      <th>month_11</th>\n","      <th>month_12</th>\n","      <th>functionality_2</th>\n","      <th>functionality_3</th>\n","      <th>functionality_4</th>\n","      <th>functionality_5</th>\n","      <th>functionality_6</th>\n","      <th>functionality_7</th>\n","      <th>functionality_8</th>\n","      <th>functionality_9</th>\n","      <th>functionality_10</th>\n","      <th>functionality_11</th>\n","      <th>functionality_12</th>\n","      <th>color_blue</th>\n","      <th>color_gold</th>\n","      <th>color_green</th>\n","      <th>color_grey</th>\n","      <th>color_none</th>\n","      <th>color_pink</th>\n","      <th>color_purple</th>\n","      <th>color_red</th>\n","      <th>color_white</th>\n","      <th>vendor_2</th>\n","      <th>vendor_3</th>\n","      <th>vendor_4</th>\n","      <th>vendor_5</th>\n","      <th>vendor_6</th>\n","      <th>vendor_7</th>\n","      <th>vendor_8</th>\n","      <th>vendor_9</th>\n","      <th>vendor_10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2016-11-14</td>\n","      <td>1</td>\n","      <td>110.0</td>\n","      <td>10.24</td>\n","      <td>9.86</td>\n","      <td>10.16</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2016-11-21</td>\n","      <td>1</td>\n","      <td>127.0</td>\n","      <td>8.27</td>\n","      <td>10.24</td>\n","      <td>9.86</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2016-11-28</td>\n","      <td>1</td>\n","      <td>84.0</td>\n","      <td>8.83</td>\n","      <td>8.27</td>\n","      <td>10.24</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2016-12-05</td>\n","      <td>1</td>\n","      <td>87.0</td>\n","      <td>8.98</td>\n","      <td>8.83</td>\n","      <td>8.27</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2016-12-12</td>\n","      <td>1</td>\n","      <td>64.0</td>\n","      <td>10.40</td>\n","      <td>8.98</td>\n","      <td>8.83</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         week  sku  weekly_sales  ...  vendor_8  vendor_9  vendor_10\n","0  2016-11-14    1         110.0  ...         0         0          0\n","1  2016-11-21    1         127.0  ...         0         0          0\n","2  2016-11-28    1          84.0  ...         0         0          0\n","3  2016-12-05    1          87.0  ...         0         0          0\n","4  2016-12-12    1          64.0  ...         0         0          0\n","\n","[5 rows x 48 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"mNTj6bUdEC_h"},"source":["Optimal decentralized model"]},{"cell_type":"markdown","metadata":{"id":"dpAGrCsrk5fU"},"source":["Train-test split function for time series (splices input arrays based on user-defined test set size)"]},{"cell_type":"code","metadata":{"id":"IlJ_0iG1kyGQ","executionInfo":{"status":"ok","timestamp":1623074124979,"user_tz":240,"elapsed":124,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}}},"source":["def train_test_split(X, y, test_size): #custom train_test_split function\n","  X_train, X_test = np.split(X, [int(len(X)*(1-test_size))])\n","  y_train, y_test = np.split(y, [int(len(y)*(1-test_size))])\n","  return X_train, X_test, y_train, y_test"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M6KTc4jpuwR5"},"source":["Defining the hyperparameter search spaces for KNN regressor and XGBoost regressor, and setting up the tuning model for Bayesian hyperparameter optimization. We keep the search spaces pretty broad because Bayesian optimization will hone in on regions of high performance and optimize around there. We want to provide an ample number of such spaces."]},{"cell_type":"code","metadata":{"id":"XtNpGZADESxr","executionInfo":{"status":"ok","timestamp":1623074127083,"user_tz":240,"elapsed":434,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}}},"source":["from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n","\n","space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n","        'gamma': hp.uniform ('gamma', 1,9),\n","        'reg_alpha' : hp.quniform('reg_alpha', 0,180,1),\n","        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n","        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n","        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n","        'n_estimators': 180,\n","        'seed': 0\n","    }\n","\n","def tune_model(space): \n","    xgb_model = xgb.XGBRegressor(n_estimators =space['n_estimators'], \n","                                 max_depth = int(space['max_depth']), \n","                                 gamma = space['gamma'],\n","                                 reg_alpha = int(space['reg_alpha']),\n","                                 min_child_weight=int(space['min_child_weight']),\n","                                colsample_bytree=int(space['colsample_bytree']),\n","                                 verbosity=0)\n","    \n","    xgb_model.fit(X_train, y_train,\n","            eval_set=[(X_train, y_train), (X_test, y_test)],\n","            early_stopping_rounds=50,\n","            verbose=False) \n","\n","    pred = xgb_model.predict(X_test)\n","    #pred = pred.reshape(len(pred),1)\n","    R2 = r2_score(y_test, pred)\n","    return {'loss': -1*R2, 'status': STATUS_OK} #trying to minimize -R2 is the same as maximizing R2"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUJliuIOl90j","executionInfo":{"status":"ok","timestamp":1623074129045,"user_tz":240,"elapsed":156,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}}},"source":["knn_space = {\n","    'n_neighbors': hp.uniform('n_neighbors', 2, 25),\n","    'weights': hp.choice('weights', ['uniform', 'distance']),\n","    'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n","    'leaf_size': hp.uniform('leaf_size', 5, 90)\n","}\n","\n","weights_values = {0: 'uniform', 1: 'distance'}\n","algorithm_values = {0: 'auto', 1: 'ball_tree', 2: 'kd_tree', 3: 'brute'}\n","\n","def tune_knn_model(space): \n","    knn_model = KNeighborsRegressor(n_neighbors=int(space['n_neighbors']), weights=space['weights'], algorithm=space['algorithm'], leaf_size=int(space['leaf_size']))\n","    knn_model.fit(X_train, y_train)\n","\n","    pred = knn_model.predict(X_test)\n","    #pred = pred.reshape(len(pred),1)\n","    R2 = r2_score(y_test, pred)\n","    return {'loss': -1*R2, 'status': STATUS_OK} #trying to minimize -R2 is the same as maximizing R2"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9_ddSaGu8Rp"},"source":["Loops through each SKU and optimizes an XGBoost regressor, KNN regressor, and ridge regression model, then returns the one with the highest R2 value for that SKU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xg0-iIMPgdew","executionInfo":{"status":"ok","timestamp":1623074838814,"user_tz":240,"elapsed":708496,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}},"outputId":"394d6962-8b2d-48ee-afe4-0d891b560b86"},"source":["np.random.seed(6)\n","best_models = dict()\n","y_test_total = []\n","y_test_pred = []\n","\n","start = time.time()\n","skus = set(sales['sku'])\n","\n","for sku in skus:\n","  max_r2 = -100 #initializing best R2 for that sku\n","  df_sku = sales[sales['sku'] == sku]\n","  X = df_sku.drop(['week', 'sku', 'weekly_sales'], axis = 1)\n","  y = df_sku[['weekly_sales']]\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","  \n","  y_test_total += list(y_test.values)\n","\n","  #XGBoostRegressor\n","  trials = Trials()\n","\n","  best_hyperparams = fmin(fn = tune_model,\n","                          space = space,\n","                          algo = tpe.suggest,\n","                          max_evals = 75,\n","                          trials = trials) #Bayesian optimization for every SKU\n","  best_hyperparams['max_depth'] = int(best_hyperparams['max_depth'])\n","  best_hyperparams['verbosity'] = 0\n","  xgb_model = xgb.XGBRegressor(**best_hyperparams)\n","  xgb_model.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], early_stopping_rounds=50, verbose=False)\n","  pred = xgb_model.predict(X_test)\n","  if r2_score(y_test, pred) > max_r2:\n","    max_r2 = r2_score(y_test, pred)\n","    y_pred_best = pred.reshape(-1, 1)\n","    best_models[sku] = xgb_model\n","\n","  #KNNRegressor\n","  trials = Trials()\n","\n","  best_hyperparams = fmin(fn = tune_knn_model,\n","                          space = knn_space,\n","                          algo = tpe.suggest,\n","                          max_evals = 75,\n","                          trials = trials) #Bayesian optimization for every SKU\n","  best_hyperparams['n_neighbors'] = int(best_hyperparams['n_neighbors'])\n","  best_hyperparams['leaf_size'] = int(best_hyperparams['leaf_size'])\n","  best_hyperparams['weights'] = weights_values[best_hyperparams['weights']]\n","  best_hyperparams['algorithm'] = algorithm_values[best_hyperparams['algorithm']]\n","\n","  knn_model = KNeighborsRegressor(**best_hyperparams)\n","  knn_model.fit(X_train, y_train)\n","  pred = knn_model.predict(X_test)\n","  if r2_score(y_test, pred) > max_r2:\n","    max_r2 = r2_score(y_test, pred)\n","    y_pred_best = pred\n","    best_models[sku] = knn_model\n","\n","  #RidgeCV\n","  ridge_model = linear_model.RidgeCV(alphas = [random.uniform(0, 2.5) for i in range(100)], scoring = 'r2') #RidgeCV takes a set of alphas, tries them all, and returns the best one using R2 as the metric\n","  ridge_model.fit(X_train, y_train)\n","\n","  pred = ridge_model.predict(X_test)\n","  if r2_score(y_test, pred) > max_r2:\n","    max_r2 = r2_score(y_test, pred)\n","    y_pred_best = pred\n","    best_models[sku] = ridge_model\n","\n","  y_test_pred += list(y_pred_best)\n","  print(f'SKU {sku} done')\n","print(f'Time to train: {time.time() - start} seconds')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["100%|██████████| 75/75 [00:11<00:00,  6.43it/s, best loss: 0.37926411616791955]\n","100%|██████████| 75/75 [00:01<00:00, 74.26it/s, best loss: 4.64857256723581]\n","SKU 1 done\n","100%|██████████| 75/75 [00:18<00:00,  4.10it/s, best loss: 0.059144238691613005]\n","100%|██████████| 75/75 [00:00<00:00, 80.00it/s, best loss: 0.6072524561891064]\n","SKU 2 done\n","100%|██████████| 75/75 [00:16<00:00,  4.63it/s, best loss: 2.0202018016934464]\n","100%|██████████| 75/75 [00:00<00:00, 75.52it/s, best loss: 2.607695061031687]\n","SKU 3 done\n","100%|██████████| 75/75 [00:14<00:00,  5.09it/s, best loss: 1.530954420948572]\n","100%|██████████| 75/75 [00:00<00:00, 87.67it/s, best loss: 2.1048644047184726]\n","SKU 4 done\n","100%|██████████| 75/75 [00:15<00:00,  4.91it/s, best loss: 1.1365232426667942]\n","100%|██████████| 75/75 [00:00<00:00, 80.89it/s, best loss: 1.4203182354483483]\n","SKU 5 done\n","100%|██████████| 75/75 [00:12<00:00,  5.86it/s, best loss: -0.03528463873511589]\n","100%|██████████| 75/75 [00:00<00:00, 81.92it/s, best loss: 0.42893400227257183]\n","SKU 6 done\n","100%|██████████| 75/75 [00:12<00:00,  6.23it/s, best loss: 0.11929762947466349]\n","100%|██████████| 75/75 [00:00<00:00, 82.50it/s, best loss: 0.04413529784789749]\n","SKU 7 done\n","100%|██████████| 75/75 [00:15<00:00,  4.88it/s, best loss: -0.3067674980323337]\n","100%|██████████| 75/75 [00:00<00:00, 82.24it/s, best loss: -0.3113105220942102]\n","SKU 8 done\n","100%|██████████| 75/75 [00:08<00:00,  8.42it/s, best loss: -0.16117424992140894]\n","100%|██████████| 75/75 [00:00<00:00, 78.40it/s, best loss: -0.13507130588827998]\n","SKU 9 done\n","100%|██████████| 75/75 [00:19<00:00,  3.85it/s, best loss: 0.5289203851054594]\n","100%|██████████| 75/75 [00:01<00:00, 73.69it/s, best loss: 0.678272785946487]\n","SKU 10 done\n","100%|██████████| 75/75 [00:18<00:00,  4.04it/s, best loss: 0.14896956093933267]\n","100%|██████████| 75/75 [00:01<00:00, 73.82it/s, best loss: -0.1594659418417954]\n","SKU 11 done\n","100%|██████████| 75/75 [00:13<00:00,  5.48it/s, best loss: -0.8493666307011206]\n","100%|██████████| 75/75 [00:00<00:00, 82.67it/s, best loss: -0.7860233279658378]\n","SKU 12 done\n","100%|██████████| 75/75 [00:17<00:00,  4.40it/s, best loss: 0.42250558231122404]\n","100%|██████████| 75/75 [00:00<00:00, 81.54it/s, best loss: 0.8912771799727295]\n","SKU 13 done\n","100%|██████████| 75/75 [00:11<00:00,  6.32it/s, best loss: -0.348397675123285]\n","100%|██████████| 75/75 [00:00<00:00, 77.10it/s, best loss: -0.5427990964586411]\n","SKU 14 done\n","100%|██████████| 75/75 [00:18<00:00,  3.96it/s, best loss: -0.2990673533333347]\n","100%|██████████| 75/75 [00:00<00:00, 80.35it/s, best loss: -0.4298182596177498]\n","SKU 15 done\n","100%|██████████| 75/75 [00:11<00:00,  6.34it/s, best loss: -0.0897575037771835]\n","100%|██████████| 75/75 [00:01<00:00, 73.51it/s, best loss: -0.19507581727877144]\n","SKU 16 done\n","100%|██████████| 75/75 [00:16<00:00,  4.53it/s, best loss: -0.5823680892293412]\n","100%|██████████| 75/75 [00:00<00:00, 85.43it/s, best loss: -0.3850861961773483]\n","SKU 17 done\n","100%|██████████| 75/75 [00:14<00:00,  5.11it/s, best loss: 0.03045596095685954]\n","100%|██████████| 75/75 [00:00<00:00, 82.53it/s, best loss: 0.1317820902825051]\n","SKU 18 done\n","100%|██████████| 75/75 [00:19<00:00,  3.83it/s, best loss: -0.20457518093720928]\n","100%|██████████| 75/75 [00:01<00:00, 64.19it/s, best loss: -0.414306800321809]\n","SKU 19 done\n","100%|██████████| 75/75 [00:14<00:00,  5.09it/s, best loss: -0.261722296864214]\n","100%|██████████| 75/75 [00:00<00:00, 80.57it/s, best loss: -0.21404456082958123]\n","SKU 20 done\n","100%|██████████| 75/75 [00:20<00:00,  3.69it/s, best loss: -0.6421084206565747]\n","100%|██████████| 75/75 [00:00<00:00, 79.05it/s, best loss: -0.34871309919812954]\n","SKU 21 done\n","100%|██████████| 75/75 [00:10<00:00,  7.45it/s, best loss: -0.004166923881428475]\n","100%|██████████| 75/75 [00:00<00:00, 80.27it/s, best loss: 0.019991160660816965]\n","SKU 22 done\n","100%|██████████| 75/75 [00:08<00:00,  9.11it/s, best loss: 0.03991846927067688]\n","100%|██████████| 75/75 [00:00<00:00, 81.66it/s, best loss: 24.229646907384986]\n","SKU 23 done\n","100%|██████████| 75/75 [00:11<00:00,  6.43it/s, best loss: 0.026004674815299778]\n","100%|██████████| 75/75 [00:00<00:00, 77.03it/s, best loss: 0.06951875300372579]\n","SKU 24 done\n","100%|██████████| 75/75 [00:20<00:00,  3.71it/s, best loss: -0.4173305126977639]\n","100%|██████████| 75/75 [00:00<00:00, 78.98it/s, best loss: -0.4779470326583808]\n","SKU 25 done\n","100%|██████████| 75/75 [00:16<00:00,  4.51it/s, best loss: -0.20958230977445147]\n","100%|██████████| 75/75 [00:00<00:00, 79.34it/s, best loss: -0.3544044615491857]\n","SKU 26 done\n","100%|██████████| 75/75 [00:18<00:00,  4.06it/s, best loss: -0.0012480890132026135]\n","100%|██████████| 75/75 [00:00<00:00, 79.81it/s, best loss: -0.0324004171814658]\n","SKU 27 done\n","100%|██████████| 75/75 [00:16<00:00,  4.51it/s, best loss: -0.19916444072816375]\n","100%|██████████| 75/75 [00:00<00:00, 80.40it/s, best loss: -0.4422035553736342]\n","SKU 28 done\n","100%|██████████| 75/75 [00:08<00:00,  8.46it/s, best loss: -0.2178522523019394]\n","100%|██████████| 75/75 [00:00<00:00, 77.34it/s, best loss: -0.5667698941148601]\n","SKU 29 done\n","100%|██████████| 75/75 [00:18<00:00,  4.04it/s, best loss: -0.3429412378476957]\n","100%|██████████| 75/75 [00:00<00:00, 78.27it/s, best loss: -0.27965516022929726]\n","SKU 30 done\n","100%|██████████| 75/75 [00:19<00:00,  3.75it/s, best loss: -0.034009319716608144]\n","100%|██████████| 75/75 [00:00<00:00, 81.97it/s, best loss: 0.0056276112057200844]\n","SKU 31 done\n","100%|██████████| 75/75 [00:17<00:00,  4.37it/s, best loss: 0.0322390911544681]\n","100%|██████████| 75/75 [00:00<00:00, 76.91it/s, best loss: -0.008513139815498394]\n","SKU 32 done\n","100%|██████████| 75/75 [00:10<00:00,  7.41it/s, best loss: -0.27311617261262044]\n","100%|██████████| 75/75 [00:00<00:00, 81.75it/s, best loss: -0.28543076139330226]\n","SKU 33 done\n","100%|██████████| 75/75 [00:12<00:00,  6.23it/s, best loss: 0.21016083525018558]\n","100%|██████████| 75/75 [00:00<00:00, 84.16it/s, best loss: 0.4842431022564917]\n","SKU 34 done\n","100%|██████████| 75/75 [00:10<00:00,  7.16it/s, best loss: -0.18205267342735365]\n","100%|██████████| 75/75 [00:00<00:00, 80.56it/s, best loss: 1.8679344942684315]\n","SKU 35 done\n","100%|██████████| 75/75 [00:11<00:00,  6.50it/s, best loss: 0.10450022578107188]\n","100%|██████████| 75/75 [00:00<00:00, 84.48it/s, best loss: 0.5201501448593722]\n","SKU 36 done\n","100%|██████████| 75/75 [00:16<00:00,  4.56it/s, best loss: 1.0019925354970711]\n","100%|██████████| 75/75 [00:00<00:00, 78.85it/s, best loss: 0.940288697926996]\n","SKU 37 done\n","100%|██████████| 75/75 [00:16<00:00,  4.64it/s, best loss: 0.33969417455011897]\n","100%|██████████| 75/75 [00:00<00:00, 79.10it/s, best loss: 2.1156995170540287]\n","SKU 38 done\n","100%|██████████| 75/75 [00:17<00:00,  4.24it/s, best loss: 0.07067302856531521]\n","100%|██████████| 75/75 [00:01<00:00, 72.85it/s, best loss: 0.06371880093219917]\n","SKU 39 done\n","100%|██████████| 75/75 [00:08<00:00,  8.76it/s, best loss: -0.4294803393512332]\n","100%|██████████| 75/75 [00:00<00:00, 86.52it/s, best loss: 4.7899695891901635]\n","SKU 40 done\n","100%|██████████| 75/75 [00:08<00:00,  9.04it/s, best loss: -0.12444541875581072]\n","100%|██████████| 75/75 [00:00<00:00, 78.85it/s, best loss: 5.555960420341711]\n","SKU 41 done\n","100%|██████████| 75/75 [00:18<00:00,  4.02it/s, best loss: 1.0273023119220315]\n","100%|██████████| 75/75 [00:00<00:00, 75.42it/s, best loss: 0.844388968140751]\n","SKU 42 done\n","100%|██████████| 75/75 [00:17<00:00,  4.27it/s, best loss: -0.03058519042898522]\n","100%|██████████| 75/75 [00:00<00:00, 85.18it/s, best loss: 0.22174195379933126]\n","SKU 43 done\n","100%|██████████| 75/75 [00:19<00:00,  3.90it/s, best loss: 0.10018751264597991]\n","100%|██████████| 75/75 [00:00<00:00, 79.32it/s, best loss: 0.13148118067185166]\n","SKU 44 done\n","Time to train: 708.3014011383057 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qngfU0IuhWtt","executionInfo":{"status":"ok","timestamp":1623074843128,"user_tz":240,"elapsed":112,"user":{"displayName":"Jake Hogan","photoUrl":"","userId":"03621424250310699854"}},"outputId":"62cb97cd-094a-4420-8cd0-b122153b5fe9"},"source":["print('Out of sample R2:',round(r2_score(np.array(y_test_total), np.array(y_test_pred)),3))\n","print('MSE:', round(mean_squared_error(np.array(y_test_total), np.array(y_test_pred)),3))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Out of sample R2: 0.667\n","MSE: 36915.227\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wdG2XRYi5z84"},"source":[""],"execution_count":null,"outputs":[]}]}